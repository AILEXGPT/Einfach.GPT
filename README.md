# üîí Einfach.PGPT üìë

> Installations- und Nutzungsdokumentation: https://docs.privategpt.dev/

Einfach.PGPT ist ein produktionsbereites KI-Projekt, das es dir erm√∂glicht, Fragen zu deinen Dokumenten mithilfe der Kraft von Gro√üen Sprachmodellen (LLMs) zu stellen, auch in Szenarien ohne Internetverbindung. 100% privat, keine Daten verlassen deine Ausf√ºhrungsumgebung zu irgendeinem Zeitpunkt.

Das Projekt bietet eine API, die alle Grundlagen bietet, um private, kontextbewusste KI-Anwendungen zu erstellen. Es folgt und erweitert den [OpenAI API-Standard](https://openai.com/blog/openai-api) und unterst√ºtzt sowohl normale als auch Streaming-Antworten.

Die API ist in zwei logische Bl√∂cke unterteilt:

**High-Level-API**, die die gesamte Komplexit√§t einer RAG (Retrieval Augmented Generation) Pipeline-Implementierung abstrahiert:
- Einlesen von Dokumenten: internes Verwalten von Dokumenten-Parsing, Aufteilung, Metadatenextraktion, Erzeugung von Einbettungen und Speicherung.
- Chat & Vervollst√§ndigungen unter Verwendung von Kontext aus eingelesenen Dokumenten: Abstraktion der Kontexterfassung, der Prompt-Engineering und der Antwortgenerierung.

**Low-Level-API**, die es fortgeschrittenen Benutzern erm√∂glicht, ihre eigenen komplexen Pipelines zu implementieren:
- Erzeugung von Einbettungen: basierend auf einem Textst√ºck.
- Kontextbezogene Chunks-Abruf: Bei einer Anfrage werden die relevantesten Textabschnitte aus den eingelesenen Dokumenten zur√ºckgegeben.

Zus√§tzlich dazu wird ein funktionierender [Gradio UI](https://www.gradio.app/) Client bereitgestellt, um die API zu testen, zusammen mit einer Reihe n√ºtzlicher Tools wie Bulk-Model-Download-Skript, Einleseskript, Dokumentenordner-√úberwachung usw.

> üëÇ **Brauchst du Hilfe bei der Anwendung von Einfach.PGPT f√ºr deinen spezifischen Anwendungsfall?**
> [Lass es uns wissen](https://forms.gle/4cSDmH13RZBHV9at7) und wir werden versuchen zu helfen! Wir verfeinern Einfach.PGPT durch dein Feedback.

## üéûÔ∏è √úberblick

### Motivation hinter Einfach.PGPT
Generative KI ist ein Game-Changer f√ºr unsere Gesellschaft, aber die Einf√ºhrung in Unternehmen aller Gr√∂√üen und datensensiblen Bereichen wie Gesundheitswesen oder Recht ist durch eine klare Sorge begrenzt: **Privatsph√§re**. Nicht sicherstellen zu k√∂nnen, dass deine Daten vollst√§ndig unter deiner Kontrolle sind, wenn du KI-Tools von Drittanbietern verwendest, ist ein Risiko, das diese Branchen nicht eingehen k√∂nnen.

### Urversion
Die erste Version von Einfach.PGPT wurde im Mai 2023 als neuartiger Ansatz eingef√ºhrt, um die Datenschutzbedenken anzugehen, indem LLMs auf eine vollst√§ndig offline Weise verwendet wurden. Dies wurde durch die Nutzung bestehender Technologien erreicht, die von der florierenden Open-Source-KI-Gemeinschaft entwickelt wurden: [LangChain](https://github.com/hwchase17/langchain), [LlamaIndex](https://www.llamaindex.ai/), [GPT4All](https://github.com/nomic-ai/gpt4all), [LlamaCpp](https://github.com/ggerganov/llama.cpp), [Chroma](https://www.trychroma.com/) und [SentenceTransformers](https://www.sbert.net/).

Diese Version, die schnell zu einem Go-to-Projekt f√ºr datenschutzsensible Setups wurde und als Grundlage f√ºr Tausende von lokal fokussierten generativen KI-Projekten diente, war der Grundstein dessen, was Einfach.PGPT heutzutage wird; also eine einfachere und lehrreichere Implementierung, um die grundlegenden Konzepte zu verstehen, die erforderlich sind, um ein vollst√§ndig lokales - und daher privates - ChatGPT-√§hnliches Tool zu erstellen.

Wenn du weiterhin damit experimentieren m√∂chtest, haben wir es im [primordial branch](https://github.com/imartinez/privateGPT/tree/primordial) des Projekts gespeichert.

> Es wird dringend empfohlen, eine saubere Klonung und Installation dieser neuen Version von Einfach.PGPT durchzuf√ºhren, wenn du von der vorherigen, urzeitlichen Version kommst.

### Gegenwart und Zukunft von Einfach.PGPT
Einfach.PGPT entwickelt sich nun zu einem Gateway f√ºr generative KI-Modelle und Grundbausteine, einschlie√ülich Vervollst√§ndigungen, Dokumenteneinlesung, RAG-Pipelines und anderen Low-Level-Bausteinen. Wir m√∂chten es jedem Entwickler erleichtern, KI-Anwendungen und -Erfahrungen zu erstellen, sowie eine geeignete umfangreiche Architektur f√ºr die Community bereitstellen, um weiter beizutragen.

Bleibe auf dem Laufenden mit unseren [Ver√∂ffentlichungen](https://github.com/imartinez/privateGPT/releases), um alle neuen Funktionen und √Ñnderungen zu sehen.

## üìÑ Dokumentation
Die vollst√§ndige Dokumentation zu Installation, Abh√§ngigkeiten, Konfiguration, Serverbetrieb, Bereitstellungsoptionen, Einlesen lokaler Dokumente, API-Details und UI-Funktionen findest du hier: https://docs.privategpt.dev/

## üß© Architektur
Konzeptionell ist Einfach.PGPT eine API, die eine RAG-Pipeline umschlie√üt und deren Grundbausteine freilegt.
* Die API wurde mit [FastAPI](https://fastapi.tiangolo.com/) erstellt und folgt dem [API-Schema von OpenAI](https://platform.openai.com/docs/api-reference).
* Die RAG-Pipeline basiert auf [LlamaIndex](https://www.llamaindex.ai/).

Das Design von Einfach.PGPT erm√∂glicht es, sowohl die API als auch die RAG-Implementierung leicht zu erweitern und anzupassen. Einige wichtige architektonische Entscheidungen sind:
* Dependency Injection, Entkopplung der verschiedenen Komponenten und Schichten.
* Verwendung von LlamaIndex-Abstraktionen wie `LLM`, `BaseEmbedding` oder `VectorStore`, was es sofort erm√∂glicht, die tats√§chlichen Implementierungen dieser Abstraktionen zu √§ndern.
* Einfachheit, so wenige Schichten und neue Abstraktionen wie m√∂glich hinzuzuf√ºgen.
* Einsatzbereit, Bereitstellung einer vollst√§ndigen Implementierung der API und RAG-Pipeline.

Hauptbausteine:
* APIs sind in `private_gpt:server:<api>` definiert. Jedes Paket enth√§lt eine `<api>_router.py` (FastAPI-Schicht) und eine `<api>_service.py` (die Service-Implementierung). Jeder *Service* verwendet LlamaIndex-Basisabstraktionen anstelle von spezifischen Implementierungen, wodurch die tats√§chliche Implementierung von ihrer Nutzung entkoppelt wird.
* Komponenten befinden sich in `private_gpt:components:<component>`. Jede *Komponente* ist daf√ºr verantwortlich, tats√§chliche Implementierungen f√ºr die Basisabstraktionen bereitzustellen, die in den Services verwendet werden - zum Beispiel ist `LLMComponent` daf√ºr verantwortlich, eine tats√§chliche Implementierung eines `LLM` bereitzustellen (zum Beispiel `LlamaCPP` oder `OpenAI`).

## üí° Mitwirken
Beitr√§ge sind willkommen! Um die Codequalit√§t zu gew√§hrleisten, haben wir mehrere Format- und Typisierungspr√ºfungen aktiviert, f√ºhre einfach `make check` vor dem Commit durch, um sicherzustellen, dass dein Code in Ordnung ist. Denke daran, deinen Code zu testen! Du findest einen Testordner mit Hilfsmitteln, und du kannst Tests mit dem Befehl `make test` ausf√ºhren.

Interessiert daran, zu Einfach.PGPT beizutragen? Wir haben die folgenden Herausforderungen vor uns, falls du helfen m√∂chtest:

### Verbesserungen
- Bessere RAG-Pipeline-Implementierung (Verbesserungen sowohl in den Indexierungs- als auch in den Abfragephasen)
- Code-Dokumentation
- Exposition von Ausf√ºhrungsparametern wie top_p, Temperatur, max_tokens... in Vervollst√§ndigungen und Chat-Vervollst√§ndigungen
- Exposition der Chunk-Gr√∂√üe in der Ingest-API
- Implementierung von Update und Delete-Dokument in der Ingest-API
- Hinzuf√ºgen von Informationen √ºber den Tokenverbrauch in jeder Antwort
- Hinzuf√ºgen zu den Vervollst√§ndigungs-APIs (Chat und Vervollst√§ndigung) der Kontextdokumente, die zur Beantwortung der Frage verwendet wurden
- Im "Modell"-Feld den tats√§chlichen Namen des LLM- oder Einbettungsmodells zur√ºckgeben

### Funktionen
- Implementierung eines Concurrency-Locks, um Fehler zu vermeiden, wenn es mehrere Aufrufe des lokalen LlamaCPP-Modells gibt
- API-Schl√ºssel-basierte Anforderungskontrolle zur API
- Unterst√ºtzung f√ºr Sagemaker
- Unterst√ºtzung von Funktionsaufrufen
- Hinzuf√ºgen von md5, um bereits eingelesene Dateien zu √ºberpr√ºfen
- Auswahl eines Dokuments zur Abfrage in der UI
- Bessere Beobachtbarkeit der RAG-Pipeline

### Projektinfrastruktur
- Verpackte Version als lokale Desktop-App (Windows-Exe, Mac-App, Linux-App)
- Dockerisierung der Anwendung f√ºr Plattformen au√üerhalb von Linux (Docker Desktop f√ºr Mac und Windows)
- Dokumentation, wie man auf AWS, GCP und Azure bereitstellt.

##
Um eine korrigierte Version der README und eine Konfiguration f√ºr das Einfach.GPT-Projekt auf Vercel mit Node und Python zu erstellen, habe ich das Repository [Einfach.GPT](https://github.com/Einfachalf/Einfach.GPT) √ºberpr√ºft. Basierend auf den verf√ºgbaren Dateien und Informationen, hier ist ein Vorschlag f√ºr die README im Markdown-Format:

---

# Einfach.GPT ‚Äì Dein pers√∂nliches ChatGPT auf deinem Computer
**14. November 2023**

## Generative KI
### Einf√ºhrung:
Im Bereich der K√ºnstlichen Intelligenz (KI), wo Datenschutz von gr√∂√üter Bedeutung ist, erweist sich Einfach.GPT als bahnbrechend. Dieses produktionsbereite KI-Projekt erm√∂glicht eine nahtlose Dokumenteninteraktion mit Large Language Models (LLMs) ohne Internetverbindung. Sein Engagement f√ºr den Datenschutz ist beispiellos ‚Äì jede Interaktion mit deinen Dokumenten findet ausschlie√ülich in deiner Ausf√ºhrungsumgebung statt, was 100% Datensicherheit gew√§hrleistet.

### Verst√§ndnis von Einfach.GPT:
Im Kern bietet Einfach.GPT eine API, die die wesentlichen Bausteine f√ºr die Erstellung privater, kontextbewusster KI-Anwendungen bereitstellt. Diese API unterst√ºtzt sowohl normale als auch Streaming-Antworten und ist vielseitig f√ºr verschiedene Anwendungen einsetzbar.

#### High-Level API: Navigation in der RAG-Pipeline:
Die API ist intelligent in zwei Hauptbl√∂cke strukturiert, beginnend mit der High-Level-API. Diese Abstraktionsschicht vereinfacht die Komplexit√§t der Retrieval Augmented Generation (RAG)-Pipeline. Wichtige Funktionen umfassen:

1. Dokumenteneinlesung:
   - Nahtlose Handhabung von Dokumenten-Parsing, -Aufteilung, Metadatenextraktion und Erzeugung von Einbettungen intern.
   - Effiziente Speichermechanismen f√ºr optimales Dokumentenmanagement.

2. Chat & Vervollst√§ndigungen:
   - Abstraktion von Kontextabruf, Prompt-Engineering und Antwortgenerierung unter Verwendung von Informationen aus eingelesenen Dokumenten.
   - Erm√∂glicht Benutzern, mit ihren Dokumenten durch nat√ºrliche Sprachanfragen zu interagieren.

#### Low-Level API: Empowerment f√ºr fortgeschrittene Benutzer:
F√ºr Benutzer, die mehr Kontrolle und Anpassung suchen, bietet die Low-Level-API die Flexibilit√§t, komplexe Pipelines zu implementieren. Wichtige Funktionen umfassen:

1. Erzeugung von Einbettungen:
   - Angepasste Einbettungserzeugung basierend auf spezifischen Textteilen, erm√∂glicht ma√ügeschneiderte KI-Interaktionen.

2. Abruf kontextueller Textbl√∂cke:
   - Bei einer Anfrage werden die relevantesten Textbl√∂cke aus den eingelesenen Dokumenten abgerufen.
   - Fortgeschrittene Benutzer k√∂nnen Anfragen f√ºr pr√§zisere Informationsabrufe feinabstimmen.

### Umfassendes Toolset: √úber die API hinaus:
Einfach.GPT bietet nicht nur eine robuste API, sondern auch ein umfassendes Toolset, einschlie√ülich:

1. Gradio UI Client:
   - Ein benutzerfreundlicher Gradio UI-Client erleichtert das Testen der API und sorgt f√ºr eine reibungslose Benutzererfahrung.

2. Utility-Skripte:
   - Eine Reihe n√ºtzlicher Tools, wie ein Bulk-Model-Download-Skript, Ingestion-Skript und ein Dokumentenordner-Watcher, vereinfachen Arbeitsabl√§ufe.

### Installation von Einfach.GPT auf deinem lokalen System: Eine Schritt-f√ºr-Schritt-Anleitung

#### Schritt 1: Klonen des Repositories
```bash
# Klonen des Einfach.GPT-Repositorys
$ git clone https://github.com/Einfachalf/Einfach.GPT
$ cd Einfach.GPT
```
Dieser Schritt beinhaltet das Herunterladen des Einfach.GPT-Codebasis vom GitHub-Repository und das Navigieren zum Projektverzeichnis.

#### Schritt 2: Installation von Python 3.11
```bash
# Erstellen einer virtuellen Umgebung mit Python 3.11
$ conda create -n Einfach.GPT python=3.11
$ conda activate Einfach.GPT
```
Dieser Schritt richtet eine virtuelle Umgebung namens `Einfach.GPT` mit Python 3.11 ein und aktiviert sie f√ºr die nachfolgenden Installationen.

#### Schritt 3: Installation von Abh√§ngigkeiten
```bash
# Installation von Poetry und Projekt-Abh√§ngigkeiten
$ brew install poetry
$ poetry install --with ui,local
```
Hier wird Poetry als Abh√§ngigkeitsmanager installiert und anschlie√üend verwendet, um die erforderlichen Projekt-Abh√§ngigkeiten zu installieren.

#### Schritt 4: Herunterladen von Einbettungs- und LLM-Modellen
```bash
# Herunterladen von Einbettungs- und Sprachmodellen
$ poetry run python scripts/setup
```
Dieser Befehl l√§dt die notwendigen Modelle herunter, was bis zu 4 GB Speicherplatz ben√∂tigen kann.

#### Schritt 5: GPU-Aktivierung (F√ºr Mac mit Metal GPU)
```bash
# Aktivierung der Metal GPU-Unterst√ºtzung
$ CMAKE_ARGS="-DLLAMA_METAL=on" pip install --force-reinstall --no-cache-dir llama-cpp-python
```
Dieser Schritt ist spezifisch f√ºr Mac mit Metal GPU. Es installiert die notwendigen Komponenten f√ºr die Metal GPU-Unterst√ºtzung.

#### Schritt 6: Starten des lokalen Servers
```bash
# Starten des lokalen Servers
$ PGPT_PROFILES=local make run
```
Dieser Befehl startet den Einfach.GPT-Lokalserver. Wenn du einen Mac mit Metal GPU verwendest, solltest du ein Log sehen, das die GPU-Nutzung anzeigt.

#### Schritt 7: Navigieren zur UI
√ñffne deinen Webbrowser und gehe zu http://localhost:8001/, um auf die Einfach.GPT-UI zuzugreifen.

Diese Schritte helfen dir, das Retrieval-augmented generation (RAG)-Framework mit Einfach.GPT lokal auf einem Mac einzurichten, einschlie√ülich der Interaktion mit Dokumenten in verschiedenen Formaten.

Wende das aus diesem Artikel gewonnene Wissen in deinen Projekten an und lass uns

 von deinen Erfahrungen wissen.

Wir sch√§tzen dein Feedback. F√ºhle dich frei, deine Gedanken, Fragen oder Vorschl√§ge im Kommentarbereich unten zu teilen.

---

### Konfiguration f√ºr Vercel mit Node und Python

Um Einfach.GPT auf Vercel zu hosten, ben√∂tigst du eine `vercel.json` Konfigurationsdatei, die angibt, wie Vercel das Projekt behandeln soll. Hier ist ein Beispiel f√ºr eine solche Konfiguration:

```json
{
  "version": 2,
  "builds": [
    {
      "src": "package.json",
      "use": "@vercel/node"
    },
    {
      "src": "pyproject.toml",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/(.*)",
      "dest": "/"
    }
  ]
}
```

Diese Konfiguration gibt an, dass Vercel sowohl Node.js- als auch Python-Abh√§ngigkeiten f√ºr das Projekt installieren und bauen soll. Die `routes`-Sektion leitet alle Anfragen an die Hauptanwendung weiter.

Stelle sicher, dass du alle erforderlichen Abh√§ngigkeiten und Skripte in deinen `package.json` und `pyproject.toml` Dateien korrekt definiert hast, damit Vercel das Projekt korrekt bauen und ausf√ºhren kann.
